from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
from datetime import datetime
import os

def run_crawler():
    print("üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu l√∫c", datetime.now().strftime("%H:%M:%S"))
    all_data = []

    driver = webdriver.Chrome()
    driver.get("https://alonhadat.com.vn/")
    driver.maximize_window()

    wait = WebDriverWait(driver, 10)

    city_dropdown = Select(wait.until(EC.presence_of_element_located((By.CLASS_NAME, "province"))))
    city_dropdown.select_by_visible_text("ƒê√† N·∫µng")

    type_dropdown = Select(wait.until(EC.presence_of_element_located((By.CLASS_NAME, "demand"))))
    type_dropdown.select_by_visible_text("Cho thu√™")

    search_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, "btnsearch")))
    search_button.click()
    time.sleep(3)

    def extract_data():
        titles = driver.find_elements(By.CSS_SELECTOR, ".ct_title a")
        descriptions = driver.find_elements(By.CSS_SELECTOR, ".content .content_description")
        areas = driver.find_elements(By.CSS_SELECTOR, ".ct_dt span")
        prices = driver.find_elements(By.CSS_SELECTOR, ".price")
        addresses = driver.find_elements(By.CSS_SELECTOR, ".address")
        images = driver.find_elements(By.CSS_SELECTOR, ".thumb img")

        for i in range(len(titles)):
            try:
                data = {
                    "Ti√™u ƒë·ªÅ": titles[i].text,
                    "M√¥ t·∫£": descriptions[i].text if i < len(descriptions) else "",
                    "Di·ªán t√≠ch": areas[i].text if i < len(areas) else "",
                    "Gi√°": prices[i].text if i < len(prices) else "",
                    "ƒê·ªãa ch·ªâ": addresses[i].text if i < len(addresses) else "",
                    "H√¨nh ·∫£nh": images[i].get_attribute("src") if i < len(images) else "",
                    "Link chi ti·∫øt": titles[i].get_attribute("href")
                }
                all_data.append(data)
            except Exception as e:
                print(f"L·ªói khi l·∫•y d·ªØ li·ªáu tin s·ªë {i}: {e}")

    extract_data()
    current_page = 1
    max_pages = 5

    while current_page < max_pages:
        try:
            next_button = wait.until(EC.presence_of_element_located((By.LINK_TEXT, ">>")))
            if "disabled" in next_button.get_attribute("class") or not next_button.is_enabled():
                break
            next_button.click()
            time.sleep(3)
            extract_data()
            current_page += 1
        except Exception as e:
            print("K·∫øt th√∫c ho·∫∑c l·ªói:", e)
            break

    date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = f"D:\\TDH_quytrinh\\real-estate-crawler\\data\\real_estate_data_{date_str}.xlsx"
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    df = pd.DataFrame(all_data)
    df.to_excel(file_path, index=False, engine='openpyxl')
    print(f"‚úÖ ƒê√£ l∆∞u {len(all_data)} d√≤ng d·ªØ li·ªáu v√†o {file_path}")
    driver.quit()


# Ch·ªù ƒë·∫øn ƒë√∫ng 06:00 ƒë·ªÉ ch·∫°y
if __name__ == "__main__":
    print("‚è∞ H·ªá th·ªëng ƒëang ch·ªù ƒë·∫øn 06:00 ƒë·ªÉ ch·∫°y crawler...")

    while True:
        now = datetime.now()
        if now.hour == 17 and now.minute == 59:
            run_crawler()
            time.sleep(70)  # Tr√°nh ch·∫°y l·∫°i nhi·ªÅu l·∫ßn trong c√πng ph√∫t
        else:
            time.sleep(30)  # Ki·ªÉm tra m·ªói 30 gi√¢y